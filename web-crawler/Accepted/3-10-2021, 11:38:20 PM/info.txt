{"id":466284940,"lang":"javascript","lang_name":"JavaScript","time":"2 years, 3 months","timestamp":1615437500,"status":10,"status_display":"Accepted","runtime":"128 ms","url":"/submissions/detail/466284940/","is_pending":"Not Pending","title":"Web Crawler","memory":"47.8 MB","code":"/**\n * // This is the HtmlParser's API interface.\n * // You should not implement it, or speculate about its implementation\n * function HtmlParser() {\n *\n *\t\t@param {string} url\n *     \t@return {string[]}\n *     \tthis.getUrls = function(url) {\n *      \t...\n *     \t};\n * };\n */\n\n/**\n * @param {string} startUrl\n * @param {HtmlParser} htmlParser\n * @return {string[]}\n*/\n\n// Intuition: We need a hashtable to track sites we've seen, and a queue to process the urls as we get them. For each url remove it from the queue, get its urls and add them to the end of the queue.\nvar crawl = function(startUrl, htmlParser) {\n    const seen = {};\n    const results = [startUrl];\n    seen[startUrl] = true;\n    \n    const hostname = getHostname(startUrl);\n    const queue = htmlParser.getUrls(startUrl);\n    \n    while(queue.length) {\n        const url = queue.shift();\n        if (!seen[url]) {\n            seen[url] = true;\n            if (getHostname(url) === hostname) {\n                results.push(url);\n                \n                const urls = htmlParser.getUrls(url);\n                for (const u of urls) {\n                    queue.push(u);\n                }\n            }\n        }\n    }\n    return results;\n};\n\nfunction getHostname(url) {\n    const hostname = (new URL(url)).hostname;\n    return hostname;\n    \n}\n\n\n\n","compare_result":"11111111111111111111","title_slug":"web-crawler","has_notes":false}